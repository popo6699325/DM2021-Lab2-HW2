{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Hsin-Ju Li(Êùé‰ø°ÂÑí)\n",
    "\n",
    "Student ID: 110033630\n",
    "\n",
    "GitHub ID: popo6699325\n",
    "\n",
    "Kaggle name: Hsin-Ju,li\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "[Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2021-Lab2-master Repo](https://github.com/fhcalderon87/DM2021-Lab2-master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/c/dm2021-lab2-hw2/) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 24th 11:59 pm, Friday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 29th 11:59 pm, Wednesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Second : Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets_DM_df = pd.read_json(\"tweets_DM.json\", lines=True)\n",
    "emotion_df = pd.read_csv(\"emotion.csv\")\n",
    "data_identification_df = pd.read_csv(\"data_identification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_source</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Snapchat'], 'tweet_id...</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['freepress', 'TrumpLeg...</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['bibleverse'], 'tweet_...</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>989</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2de2...</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>827</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['mixedfeeling', 'butim...</td>\n",
       "      <td>2015-05-12 12:51:52</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>368</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x29d0...</td>\n",
       "      <td>2017-10-02 17:54:04</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>498</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2a6a...</td>\n",
       "      <td>2016-10-10 11:04:32</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>840</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x24fa...</td>\n",
       "      <td>2016-09-02 14:25:06</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>360</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Sundayvibes'], 'tweet...</td>\n",
       "      <td>2016-11-16 01:40:07</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         _score          _index  \\\n",
       "0           391  hashtag_tweets   \n",
       "1           433  hashtag_tweets   \n",
       "2           232  hashtag_tweets   \n",
       "3           376  hashtag_tweets   \n",
       "4           989  hashtag_tweets   \n",
       "...         ...             ...   \n",
       "1867530     827  hashtag_tweets   \n",
       "1867531     368  hashtag_tweets   \n",
       "1867532     498  hashtag_tweets   \n",
       "1867533     840  hashtag_tweets   \n",
       "1867534     360  hashtag_tweets   \n",
       "\n",
       "                                                   _source  \\\n",
       "0        {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
       "1        {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
       "2        {'tweet': {'hashtags': ['bibleverse'], 'tweet_...   \n",
       "3        {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
       "4        {'tweet': {'hashtags': [], 'tweet_id': '0x2de2...   \n",
       "...                                                    ...   \n",
       "1867530  {'tweet': {'hashtags': ['mixedfeeling', 'butim...   \n",
       "1867531  {'tweet': {'hashtags': [], 'tweet_id': '0x29d0...   \n",
       "1867532  {'tweet': {'hashtags': [], 'tweet_id': '0x2a6a...   \n",
       "1867533  {'tweet': {'hashtags': [], 'tweet_id': '0x24fa...   \n",
       "1867534  {'tweet': {'hashtags': ['Sundayvibes'], 'tweet...   \n",
       "\n",
       "                  _crawldate   _type  \n",
       "0        2015-05-23 11:42:47  tweets  \n",
       "1        2016-01-28 04:52:09  tweets  \n",
       "2        2017-12-25 04:39:20  tweets  \n",
       "3        2016-01-24 23:53:05  tweets  \n",
       "4        2016-01-08 17:18:59  tweets  \n",
       "...                      ...     ...  \n",
       "1867530  2015-05-12 12:51:52  tweets  \n",
       "1867531  2017-10-02 17:54:04  tweets  \n",
       "1867532  2016-10-10 11:04:32  tweets  \n",
       "1867533  2016-09-02 14:25:06  tweets  \n",
       "1867534  2016-11-16 01:40:07  tweets  \n",
       "\n",
       "[1867535 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_DM_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x38dba0</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x300ea2</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x360b99</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x22eecf</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x2fb282</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id       emotion\n",
       "0        0x3140b1       sadness\n",
       "1        0x368b73       disgust\n",
       "2        0x296183  anticipation\n",
       "3        0x2bd6e1           joy\n",
       "4        0x2ee1dd  anticipation\n",
       "...           ...           ...\n",
       "1455558  0x38dba0           joy\n",
       "1455559  0x300ea2           joy\n",
       "1455560  0x360b99          fear\n",
       "1455561  0x22eecf           joy\n",
       "1455562  0x2fb282  anticipation\n",
       "\n",
       "[1455563 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x227e25</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x293813</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x1e1a7e</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x2156a5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x2bb9d2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification\n",
       "0        0x28cc61           test\n",
       "1        0x29e452          train\n",
       "2        0x2b3819          train\n",
       "3        0x2db41f           test\n",
       "4        0x2a2acc          train\n",
       "...           ...            ...\n",
       "1867530  0x227e25          train\n",
       "1867531  0x293813          train\n",
       "1867532  0x1e1a7e          train\n",
       "1867533  0x2156a5          train\n",
       "1867534  0x2bb9d2          train\n",
       "\n",
       "[1867535 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_identification_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = []\n",
    "id = []\n",
    "text = []\n",
    "for source in tweets_DM_df['_source']:\n",
    "    hashtags.append(source['tweet']['hashtags'])\n",
    "    id.append(source['tweet']['tweet_id'])\n",
    "    text.append(source['tweet']['text'])\n",
    "tweets_DM_df['hashtags'] = hashtags\n",
    "tweets_DM_df['tweet_id'] = id\n",
    "tweets_DM_df['text'] = text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DM_df.drop(columns='_index', inplace=True)\n",
    "tweets_DM_df.drop(columns='_source', inplace=True)\n",
    "tweets_DM_df.drop(columns='_crawldate', inplace=True)\n",
    "tweets_DM_df.drop(columns='_type', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>989</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>827</td>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>368</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>498</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>840</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>360</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         _score                         hashtags  tweet_id  \\\n",
       "0           391                       [Snapchat]  0x376b20   \n",
       "1           433    [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2           232                     [bibleverse]  0x28b412   \n",
       "3           376                               []  0x1cd5b0   \n",
       "4           989                               []  0x2de201   \n",
       "...         ...                              ...       ...   \n",
       "1867530     827  [mixedfeeling, butimTHATperson]  0x316b80   \n",
       "1867531     368                               []  0x29d0cb   \n",
       "1867532     498                               []  0x2a6a4f   \n",
       "1867533     840                               []  0x24faed   \n",
       "1867534     360                    [Sundayvibes]  0x34be8c   \n",
       "\n",
       "                                                      text  \n",
       "0        People who post \"add me on #Snapchat\" must be ...  \n",
       "1        @brianklaas As we see, Trump is dangerous to #...  \n",
       "2        Confident of your obedience, I write to you, k...  \n",
       "3                      Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>  \n",
       "4        \"Trust is not the same as faith. A friend is s...  \n",
       "...                                                    ...  \n",
       "1867530  When you buy the last 2 tickets remaining for ...  \n",
       "1867531  I swear all this hard work gone pay off one da...  \n",
       "1867532  @Parcel2Go no card left when I wasn't in so I ...  \n",
       "1867533  Ah, corporate life, where you can date <LH> us...  \n",
       "1867534             Blessed to be living #Sundayvibes <LH>  \n",
       "\n",
       "[1867535 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_DM_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=data_identification_df[data_identification_df['identification'] == 'test']\n",
    "train_df=data_identification_df[data_identification_df['identification'] == 'train']\n",
    "train_data_df = tweets_DM_df[tweets_DM_df.tweet_id.isin(train_df.tweet_id)].reset_index(drop=True)\n",
    "test_data_df = tweets_DM_df[tweets_DM_df.tweet_id.isin(test_df.tweet_id)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = train_data_df.sort_values(by=['tweet_id']).reset_index(drop=True)\n",
    "emotion_df = emotion_df.sort_values(by=['tweet_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['emotion'] = emotion_df.emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>242</td>\n",
       "      <td>[BlackMirror]</td>\n",
       "      <td>0x1c7f10</td>\n",
       "      <td>o m g Shut Up And Dance though #BlackMirror &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>915</td>\n",
       "      <td>[twitch, Destinybeta, Destiny, Destiny2, Desti...</td>\n",
       "      <td>0x1c7f11</td>\n",
       "      <td>On #twitch &lt;LH&gt; on the #Destinybeta #Destiny #...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>939</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x1c7f14</td>\n",
       "      <td>A nice sunny wak this morning not many &lt;LH&gt; ar...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181</td>\n",
       "      <td>[Confession, NationalCandyCornDay, CouldEatThe...</td>\n",
       "      <td>0x1c7f15</td>\n",
       "      <td>I'm one of those people who love candy corn......</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>970</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x1c7f16</td>\n",
       "      <td>@metmuseum What are these? They look like some...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>922</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x38fe18</td>\n",
       "      <td>@LJPBR @FifthHarmony Um  My vote For @FifthHar...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>77</td>\n",
       "      <td>[WesHoolahan, WALvIRL, COYBIG]</td>\n",
       "      <td>0x38fe19</td>\n",
       "      <td>Where is #WesHoolahan?!  #WALvIRL #COYBIG &lt;LH&gt;</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>25</td>\n",
       "      <td>[not, maga]</td>\n",
       "      <td>0x38fe1a</td>\n",
       "      <td>@mattmfm Fake news! &lt;LH&gt; propagated by Tumpkin...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>639</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x38fe1c</td>\n",
       "      <td>..today was brutal  ..#Hungover</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>630</td>\n",
       "      <td>[redheadproblems, ouch, burnt]</td>\n",
       "      <td>0x38fe1d</td>\n",
       "      <td>Love it when I sun burn my forehead!! NOT!! üò´üò±...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         _score                                           hashtags  tweet_id  \\\n",
       "0           242                                      [BlackMirror]  0x1c7f10   \n",
       "1           915  [twitch, Destinybeta, Destiny, Destiny2, Desti...  0x1c7f11   \n",
       "2           939                                                 []  0x1c7f14   \n",
       "3           181  [Confession, NationalCandyCornDay, CouldEatThe...  0x1c7f15   \n",
       "4           970                                                 []  0x1c7f16   \n",
       "...         ...                                                ...       ...   \n",
       "1455558     922                                                 []  0x38fe18   \n",
       "1455559      77                     [WesHoolahan, WALvIRL, COYBIG]  0x38fe19   \n",
       "1455560      25                                        [not, maga]  0x38fe1a   \n",
       "1455561     639                                                 []  0x38fe1c   \n",
       "1455562     630                     [redheadproblems, ouch, burnt]  0x38fe1d   \n",
       "\n",
       "                                                      text       emotion  \n",
       "0         o m g Shut Up And Dance though #BlackMirror <LH>           joy  \n",
       "1        On #twitch <LH> on the #Destinybeta #Destiny #...  anticipation  \n",
       "2        A nice sunny wak this morning not many <LH> ar...           joy  \n",
       "3        I'm one of those people who love candy corn......           joy  \n",
       "4        @metmuseum What are these? They look like some...       disgust  \n",
       "...                                                    ...           ...  \n",
       "1455558  @LJPBR @FifthHarmony Um  My vote For @FifthHar...       sadness  \n",
       "1455559     Where is #WesHoolahan?!  #WALvIRL #COYBIG <LH>  anticipation  \n",
       "1455560  @mattmfm Fake news! <LH> propagated by Tumpkin...      surprise  \n",
       "1455561                    ..today was brutal  ..#Hungover       disgust  \n",
       "1455562  Love it when I sun burn my forehead!! NOT!! üò´üò±...       sadness  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>989</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>[GodsPlan, GodsWork]</td>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>310</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x26289a</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411967</th>\n",
       "      <td>602</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>\"For this is the message that ye heard from th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411968</th>\n",
       "      <td>598</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>\"There is a lad here, which hath five barley l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411969</th>\n",
       "      <td>827</td>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411970</th>\n",
       "      <td>368</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411971</th>\n",
       "      <td>498</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        _score                           hashtags  tweet_id  \\\n",
       "0          232                       [bibleverse]  0x28b412   \n",
       "1          989                                 []  0x2de201   \n",
       "2           66  [materialism, money, possessions]  0x218443   \n",
       "3          104               [GodsPlan, GodsWork]  0x2939d5   \n",
       "4          310                                 []  0x26289a   \n",
       "...        ...                                ...       ...   \n",
       "411967     602                                 []  0x2913b4   \n",
       "411968     598                                 []  0x2a980e   \n",
       "411969     827    [mixedfeeling, butimTHATperson]  0x316b80   \n",
       "411970     368                                 []  0x29d0cb   \n",
       "411971     498                                 []  0x2a6a4f   \n",
       "\n",
       "                                                     text  \n",
       "0       Confident of your obedience, I write to you, k...  \n",
       "1       \"Trust is not the same as faith. A friend is s...  \n",
       "2       When do you have enough ? When are you satisfi...  \n",
       "3       God woke you up, now chase the day #GodsPlan #...  \n",
       "4       In these tough times, who do YOU turn to as yo...  \n",
       "...                                                   ...  \n",
       "411967  \"For this is the message that ye heard from th...  \n",
       "411968  \"There is a lad here, which hath five barley l...  \n",
       "411969  When you buy the last 2 tickets remaining for ...  \n",
       "411970  I swear all this hard work gone pay off one da...  \n",
       "411971  @Parcel2Go no card left when I wasn't in so I ...  \n",
       "\n",
       "[411972 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "anger            39867\n",
       "anticipation    248935\n",
       "disgust         139101\n",
       "fear             63999\n",
       "joy             516017\n",
       "sadness         193437\n",
       "surprise         48729\n",
       "trust           205478\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.groupby(['emotion']).count()['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Undersampling (All class size are same as anger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr_train_anger = np.array(train_data_df[train_data_df.emotion == 'anger'])\n",
    "arr_train_anticipation = np.array(train_data_df[train_data_df.emotion == 'anticipation'])\n",
    "arr_train_disgust = np.array(train_data_df[train_data_df.emotion == 'disgust'])\n",
    "arr_train_fear = np.array(train_data_df[train_data_df.emotion == 'fear'])\n",
    "arr_train_joy = np.array(train_data_df[train_data_df.emotion == 'joy'])\n",
    "arr_train_sadness = np.array(train_data_df[train_data_df.emotion == 'sadness'])\n",
    "arr_train_surprise = np.array(train_data_df[train_data_df.emotion == 'surprise'])\n",
    "arr_train_trust = np.array(train_data_df[train_data_df.emotion == 'trust'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(arr_train_anger)\n",
    "np.random.shuffle(arr_train_anticipation)\n",
    "np.random.shuffle(arr_train_disgust)\n",
    "np.random.shuffle(arr_train_fear)\n",
    "np.random.shuffle(arr_train_joy)\n",
    "np.random.shuffle(arr_train_sadness)\n",
    "np.random.shuffle(arr_train_surprise)\n",
    "np.random.shuffle(arr_train_trust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(arr_train_anger)\n",
    "new_arr_train_anger = arr_train_anger[:size,:]\n",
    "new_arr_train_anticipation = arr_train_anticipation[:size,:]\n",
    "new_arr_train_disgust = arr_train_disgust[:size,:]\n",
    "new_arr_train_fear = arr_train_fear[:size,:]\n",
    "new_arr_train_joy = arr_train_joy[:size,:]\n",
    "new_arr_train_sadness = arr_train_sadness[:size,:]\n",
    "new_arr_train_surprise = arr_train_surprise[:size,:]\n",
    "new_arr_train_trust = arr_train_trust[:size,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = train_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = np.concatenate((new_arr_train_anger, new_arr_train_anticipation, new_arr_train_disgust, new_arr_train_fear, new_arr_train_joy,\n",
    "                                new_arr_train_sadness, new_arr_train_surprise, new_arr_train_trust), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(train_data_df)\n",
    "train_data_df = pd.DataFrame(train_data_df, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\popo6\\Anaconda3\\envs\\datamining\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "BOW_vectorizer = CountVectorizer(tokenizer=nltk.word_tokenize)\n",
    "BOW_vectorizer.fit(train_data_df.text)\n",
    "\n",
    "train_data_BOW_features = BOW_vectorizer.transform(train_data_df.text)\n",
    "test_data_BOW_features = BOW_vectorizer.transform(test_data_df.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1455563, 994692)\n",
      "(411972, 994692)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_BOW_features.shape)\n",
    "print(test_data_BOW_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['!', '#', '$', '%', '&', \"'\", \"''\", \"''alhamdulillah\", \"''and\",\n",
       "       \"''be\", \"''behold\", \"''believe\", \"''brands\", \"''care\", \"''darlin\",\n",
       "       \"''do\", \"''dream\", \"''encounters\", \"''epic\", \"''family\", \"''folks\",\n",
       "       \"''for\", \"''funnel\", \"''have\", \"''if\", \"''in\", \"''know\", \"''learn\",\n",
       "       \"''live\", \"''looking\", \"''man\", \"''me\", \"''new\", \"''not\", \"''now\",\n",
       "       \"''others\", \"''provide\", \"''say\", \"''so\", \"''sometimes\",\n",
       "       \"''thanks\", \"''that\", \"''the\", \"''there\", \"''they\", \"''this\",\n",
       "       \"''tis\", \"''to\", \"''told\", \"''twas\", \"''very\", \"''when\", \"''where\",\n",
       "       \"''whose\", \"''you\", \"'-\", \"'-d\", \"'-mahek.dave\", \"'-not\", \"'/\",\n",
       "       \"'00\", \"'000s\", \"'007\", \"'02\", \"'03\", \"'04\", \"'04/\", \"'05\", \"'06\",\n",
       "       \"'06‚Äîtv\", \"'07\", \"'08\", \"'09\", \"'0wner_assad\", \"'10\", \"'101\",\n",
       "       \"'10k\", \"'11\", \"'12\", \"'13\", \"'13/9/2017\", \"'14\", \"'14th\", \"'15\",\n",
       "       \"'16\", \"'17\", \"'17-18\", \"'17/'18\", \"'18\", \"'19\", \"'1984\", \"'19s\",\n",
       "       \"'1st\", \"'20\", \"'20/20\", \"'2017\", \"'21\", \"'22\", \"'24\", \"'25\",\n",
       "       \"'254\", \"'28\", \"'2shits\", \"'31/8/2017\", \"'3rd\", \"'3s\", \"'3some\",\n",
       "       \"'44\", \"'45\", \"'46\", \"'48\", \"'4last\", \"'50\", \"'500\", \"'50s\", \"'52\",\n",
       "       \"'56\", \"'5oz\", \"'5pm\", \"'61\", \"'66\", \"'67\", \"'68\", \"'69\", \"'70s\",\n",
       "       \"'70s-themed\", \"'70s.\", \"'74\", \"'75\", \"'76\", \"'77\", \"'78\", \"'80\",\n",
       "       \"'80s\", \"'82-'96\", \"'83\", \"'84\", \"'85\", \"'87\", \"'88\", \"'89\", \"'90\",\n",
       "       \"'90s\", \"'91\", \"'92\", \"'93\", \"'94\", \"'96\", \"'96üòâüòò‚ò∫‚ù§\", \"'97\", \"'98\",\n",
       "       \"'99\", \"'^^\", \"'aave\", \"'abbas\", \"'abbreviated\", \"'abdicate\",\n",
       "       \"'able\", \"'abode\", \"'above\", \"'absolutely\", \"'absolution\",\n",
       "       \"'abuser\", \"'accept\", \"'access\", \"'accidentally\", \"'accomplished\",\n",
       "       \"'accomplishing\", \"'accomplishment\", \"'according\", \"'acquisition\",\n",
       "       \"'acrylic\", \"'actions\", \"'actor\", \"'actress\", \"'acts\", \"'ad\",\n",
       "       \"'adaptation\", \"'addiction\", \"'adjust\", \"'administration\",\n",
       "       \"'admission\", \"'admitted\", \"'adopted\", \"'adult\", \"'adults\",\n",
       "       \"'advance\", \"'adventures\", \"'affective\", \"'afraid\", \"'after\",\n",
       "       \"'afternoon\", \"'again\", \"'against\", \"'agents\", \"'ahab\", \"'ahoy\",\n",
       "       \"'ai\", \"'aim\", \"'aina\"], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_name = BOW_vectorizer.get_feature_names_out()\n",
    "feature_name[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train = train_data_BOW_features\n",
    "Y_train = train_data_df.emotion\n",
    "\n",
    "X_test = test_data_BOW_features\n",
    "\n",
    "clf_decisiontree = DecisionTreeClassifier(max_depth=10000)\n",
    "clf_decisiontree = clf_decisiontree.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_pred = clf_decisiontree.predict(X_train)\n",
    "Y_test_pred = clf_decisiontree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9964803996803986\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       1.00      1.00      1.00     39867\n",
      "anticipation       1.00      1.00      1.00    248935\n",
      "     disgust       1.00      1.00      1.00    139101\n",
      "        fear       1.00      0.99      0.99     63999\n",
      "         joy       0.99      1.00      1.00    516017\n",
      "     sadness       1.00      1.00      1.00    193437\n",
      "    surprise       1.00      0.99      1.00     48729\n",
      "       trust       1.00      0.99      0.99    205478\n",
      "\n",
      "    accuracy                           1.00   1455563\n",
      "   macro avg       1.00      1.00      1.00   1455563\n",
      "weighted avg       1.00      1.00      1.00   1455563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(accuracy_score(y_true=Y_train, y_pred=Y_train_pred))\n",
    "print(classification_report(y_true=Y_train, y_pred=Y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = {'id':test_data_df.tweet_id, 'emotion':Y_test_pred}\n",
    "test_pred = pd.DataFrame(test_pred)\n",
    "test_pred.to_csv('Prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\popo6\\Anaconda3\\envs\\datamining\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clf_decisiontree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27676/3884197982.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mclf_MLP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_MLP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mY_train_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_decisiontree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mY_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_decisiontree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clf_decisiontree' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X_train = train_data_BOW_features\n",
    "Y_train = train_data_df.emotion\n",
    "\n",
    "X_test = test_data_BOW_features\n",
    "\n",
    "clf_MLP = MLPClassifier(hidden_layer_sizes=(200,100,50), activation='relu', solver='adam', batch_size='auto',\n",
    "                        learning_rate_init=0.01, max_iter = 2000, shuffle=True, tol=1e-3, random_state=1, early_stopping=True)\n",
    "clf_MLP = clf_MLP.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_pred = clf_MLP.predict(X_train)\n",
    "Y_test_pred = clf_MLP.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred = clf_MLP.predict(X_train)\n",
    "Y_test_pred = clf_MLP.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63872329813275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.71      0.27      0.39     39867\n",
      "anticipation       0.71      0.66      0.68    248935\n",
      "     disgust       0.51      0.57      0.54    139101\n",
      "        fear       0.66      0.56      0.61     63999\n",
      "         joy       0.63      0.86      0.73    516017\n",
      "     sadness       0.57      0.58      0.57    193437\n",
      "    surprise       0.87      0.27      0.41     48729\n",
      "       trust       0.82      0.34      0.48    205478\n",
      "\n",
      "    accuracy                           0.64   1455563\n",
      "   macro avg       0.69      0.51      0.55   1455563\n",
      "weighted avg       0.66      0.64      0.62   1455563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(accuracy_score(y_true=Y_train, y_pred=Y_train_pred))\n",
    "print(classification_report(y_true=Y_train, y_pred=Y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = {'id':test_data_df.tweet_id, 'emotion':Y_test_pred}\n",
    "test_pred = pd.DataFrame(test_pred)\n",
    "test_pred.to_csv('Prediction_MLP.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning use keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data_BOW_features\n",
    "Y_train = train_data_df.emotion\n",
    "\n",
    "X_test = test_data_BOW_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " 0             joy\n",
      "1    anticipation\n",
      "2             joy\n",
      "3             joy\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1455563,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:4]:\n",
      " [[0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "\n",
      "y_train.shape:  (1455563, 8)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(Y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', Y_train[0:4])\n",
    "print('\\ny_train.shape: ', Y_train.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, Y_train)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  994692\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 994692)]          0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               99469300  \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 99,476,284\n",
      "Trainable params: 99,476,284\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # input size\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=100)(X)  # 100\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 8 output size\n",
    "H4 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H4\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "45487/45487 [==============================] - 1581s 35ms/step - loss: 0.3302 - accuracy: 0.8906\n",
      "Epoch 2/6\n",
      "45487/45487 [==============================] - 1526s 33ms/step - loss: 0.3065 - accuracy: 0.8984\n",
      "Epoch 3/6\n",
      "45487/45487 [==============================] - 1521s 33ms/step - loss: 0.2885 - accuracy: 0.9045 - loss: 0.2885 - ac\n",
      "Epoch 4/6\n",
      "45487/45487 [==============================] - 1522s 33ms/step - loss: 0.2739 - accuracy: 0.9093\n",
      "Epoch 5/6\n",
      "45487/45487 [==============================] - 1521s 33ms/step - loss: 0.2609 - accuracy: 0.9137\n",
      "Epoch 6/6\n",
      "45487/45487 [==============================] - 1523s 33ms/step - loss: 0.2497 - accuracy: 0.91741:01 - loss: 0.2486 - accu\n",
      "training finish\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 6\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger])\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.30865769e-08, 9.96636391e-01, 1.86843550e-07, 2.56285615e-09,\n",
       "        1.03184127e-03, 1.97518489e-06, 9.79381002e-06, 2.31968984e-03],\n",
       "       [1.06304565e-08, 9.15955976e-02, 2.52464565e-06, 1.63969034e-05,\n",
       "        8.13233036e-09, 5.65887922e-06, 1.34613627e-04, 9.08245265e-01],\n",
       "       [3.79486079e-03, 1.32101148e-01, 2.08835816e-03, 1.35375308e-02,\n",
       "        2.60755420e-01, 5.86686313e-01, 7.43859098e-04, 2.92486919e-04],\n",
       "       [8.56897736e-04, 4.90468264e-01, 9.51050388e-05, 3.09741794e-04,\n",
       "        3.00378054e-01, 2.42723436e-05, 3.50386440e-03, 2.04363823e-01],\n",
       "       [2.26804535e-04, 5.35527587e-01, 7.17054296e-04, 2.70781443e-02,\n",
       "        2.95014004e-03, 2.79688742e-02, 2.00747736e-02, 3.85456592e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## predict\n",
    "Y_test_pred = model.predict(X_test, batch_size=128)\n",
    "Y_test_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anticipation', 'trust', 'sadness', 'anticipation', 'anticipation'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_pred = label_decode(label_encoder, Y_test_pred)\n",
    "Y_test_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred = model.predict(X_train, batch_size=128)\n",
    "Y_train_pred = label_decode(label_encoder, Y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.932705077004568\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.96      0.83      0.89     39867\n",
      "anticipation       0.94      0.94      0.94    248935\n",
      "     disgust       0.93      0.93      0.93    139101\n",
      "        fear       0.95      0.89      0.92     63999\n",
      "         joy       0.92      0.97      0.94    516017\n",
      "     sadness       0.92      0.93      0.93    193437\n",
      "    surprise       0.95      0.86      0.90     48729\n",
      "       trust       0.94      0.90      0.92    205478\n",
      "\n",
      "    accuracy                           0.93   1455563\n",
      "   macro avg       0.94      0.91      0.92   1455563\n",
      "weighted avg       0.93      0.93      0.93   1455563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Accuracy : \"+str(accuracy_score(y_true=Y_train, y_pred=Y_train_pred)))\n",
    "print(classification_report(y_true=Y_train, y_pred=Y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = {'id':test_data_df.tweet_id, 'emotion':Y_test_pred}\n",
    "test_pred = pd.DataFrame(test_pred)\n",
    "test_pred.to_csv('Prediction_keras.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Third : A report of my work developping in the model for the competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I divide three part to do, data classification, data process and different way to train model.\n",
    "    - In data classification part, use pandas to read the three files including ‚Äútweets_DM.json‚Äù, ‚Äúemotion.csv‚Äù and ‚Äúdata_identification.csv‚Äù.\n",
    "        1. In file ‚Äútweets_DM.json‚Äù we have interest in _source column and in that column we need to separate ‚Äútweet_id‚Äù ,‚Äùhashtags‚Äù and ‚Äútext‚Äù. Additionally, I preserve information of score.\n",
    "        2. In file ‚Äúdata_indentification.csv‚Äù I separate the put the same information to a pandas.dataframe according to train ant test. Then I use the method(isin) of pandas.dataframe to distinguish training data and testing data and put together respectively. So, now we have training data and testing data separately.\n",
    "        3. Because we need to discriminate what sentiment it is according a tweet information, we have file ‚Äúemotion.csv‚Äù that is for training data. Use previous processed training data and emotion.csv, then sort them in terms of ‚Äútweet_id‚Äù and concatenate two dataframe.\n",
    "        4. Finally we have preliminary data about train and test. \n",
    "    \n",
    "    - In data process part, I try to let the data is balance and tokenize text to train model.\n",
    "        1. I think balanced data to train model is better than unbalanced data and the training data up to 1.4 million records. So, I don‚Äô use oversampling but use undersampling to get data balanced.\n",
    "        2. Then, use package countvertorizer to token text let me to get word information. I found it have one hundred thousand features. It‚Äôs so big. I will try to reduce them to train and compare accuracy of different features.\n",
    "\n",
    "    - In train model, I use three kinds of model, decision tree, multi-layer perceptron using sklearn and neural network using keras, to compare and predict. Then use some package to see the performance matrix\n",
    "\n",
    "        1. I use decisiontree model by package sklearn to train model, I change the parameter ‚Äúmax_depth‚Äù to compare the result. I set 100, 1000, 10000, 30000 and unlimit it. In testing data, the best result is 0.35732 by max_depth equal to 100. The other results are lower. Otherwise, I don‚Äôt reduce features and use all training data in previous results. Now, I reduce features from one hundred thousand to fifty thousand but I get worse results.\n",
    "\n",
    "        2.  Then I use neural network by package keras to train model and predict. The parameters set same as TA teached, the testing score I get 0.4294. It improves obviously. I try to set different parameter, like the number of hidden layers, size of hidden layers, epochs and batch size. I can get the best score 0.4429 when two hidden layer about 100 and 64 neurons, epochs equal to 8 and batch size equal to 32. The same is that I use all training data and not reduce features to train. Now, I try to balance data using undersampling according to the smallest number of anger class but I get the worst score 0.30327.\n",
    "\n",
    "        3.  Finally, I also use multi-layer perceptron by package sklearn. It take me a lot of time to train, I interrupt the training model when it train 5471 minutes. I don‚Äôt sure if I set very strict parameters let it unable to complete the training. Surprisingly, it gets the highest score 0.46807 although I use all training data and all features. It takes too much time, I can‚Äôt train it using different parameters. \n",
    "* Conclusion :\n",
    "    1.  If I use all training data and all feature to train, it can have a good result. Moreover, multi-layer perceptron by sklearn is best model in three kinds of model but it takes too much time. Maybe the parameters I set in others model are not well, I think there must be more suitable parameters to get the greater results.\n",
    "\n",
    "    2.  On the other hand, in data process part I try to undersampling data to get balanced data. But the number of anger is too lower, the remaining training data is too low even lower than testing data. I should process balanced data using better method, maybe as balanced as possible without throwing away data too much. Or setting a threshold , then number exceeds threshold to reduce number, on the contrary, duplicate data or do similar fake data.\n",
    "\n",
    "    3.  In features dimension, it is too higher. I should do some method to handle the text data. Close to deadline, I found that there are some unncecessary information in the text like URL. If I can delete those, maybe dimension will reduce greatly. In addition, using different word processing might allow the model to train better. Maybe try word2vec to make words related to each other, I guess it will make model train better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
